{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmartGuide Obstacle Classification Model Training\n",
    "\n",
    "This notebook contains the complete workflow for:\n",
    "1. Loading and preprocessing the ultrasonic sensor data\n",
    "2. Feature engineering and extraction\n",
    "3. Model architecture design and training\n",
    "4. Model evaluation and optimization\n",
    "5. Conversion to TensorFlow Lite for microcontrollers\n",
    "6. Generating the C++ model header file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "Load the collected sensor data from CSV files and prepare it for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# Define paths to data folders\n",
    "data_root = \"../data/obstacle_data/\"\n",
    "obstacle_types = ['wall', 'person', 'chair', 'table', 'stairs', 'door', 'pole', 'other']\n",
    "\n",
    "# Function to load and preprocess a data file\n",
    "def load_data_file(file_path, label):\n",
    "    # Load CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract relevant columns\n",
    "    lower_sensor = df['lower_distance']\n",
    "    upper_sensor = df['upper_distance']\n",
    "    \n",
    "    # Normalize values\n",
    "    lower_sensor = np.clip(lower_sensor, 0, 400) / 400.0\n",
    "    upper_sensor = np.clip(upper_sensor, 0, 400) / 400.0\n",
    "    \n",
    "    # Create sequences of length 8\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    # Slide a window of 8 readings\n",
    "    for i in range(len(lower_sensor) - 8):\n",
    "        sequence = []\n",
    "        for j in range(8):\n",
    "            # Calculate features for this time step\n",
    "            features = extract_features(lower_sensor[i+j], upper_sensor[i+j], \n",
    "                                       lower_sensor[i+j-1] if j > 0 else lower_sensor[i+j],\n",
    "                                       upper_sensor[i+j-1] if j > 0 else upper_sensor[i+j])\n",
    "            sequence.append(features)\n",
    "        \n",
    "        sequences.append(sequence)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return sequences, labels\n",
    "\n",
    "# Define feature extraction function\n",
    "def extract_features(lower, upper, prev_lower, prev_upper):\n",
    "    features = [\n",
    "        lower,                      # Lower sensor reading\n",
    "        upper,                      # Upper sensor reading\n",
    "        abs(lower - upper),         # Height difference\n",
    "        (lower + upper) / 2.0,      # Average distance\n",
    "        lower - prev_lower,         # Rate of change (lower)\n",
    "        upper - prev_upper,         # Rate of change (upper)\n",
    "        0,                          # Placeholder for variance\n",
    "        0,                          # Placeholder for frequency\n",
    "        min(lower, upper),          # Minimum distance\n",
    "        max(lower, upper)           # Maximum distance\n",
    "    ]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "# Load all data files\n",
    "all_sequences = []\n",
    "all_labels = []\n",
    "\n",
    "for i, obstacle_type in enumerate(obstacle_types):\n",
    "    obstacle_files = glob.glob(os.path.join(data_root, obstacle_type, \"*.csv\"))\n",
    "    print(f\"Loading {len(obstacle_files)} files for {obstacle_type}\")\n",
    "    \n",
    "    for file_path in obstacle_files:\n",
    "        sequences, labels = load_data_file(file_path, i)\n",
    "        all_sequences.extend(sequences)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(all_sequences)\n",
    "y = np.array(all_labels)\n",
    "\n",
    "print(f\"Total dataset: {X.shape}, Labels: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Further split training data to create validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "# Reshape for normalization\n",
    "original_shape = X_train.shape\n",
    "X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "X_val_reshaped = X_val.reshape(-1, X_val.shape[-1])\n",
    "X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_reshaped = scaler.fit_transform(X_train_reshaped)\n",
    "X_val_reshaped = scaler.transform(X_val_reshaped)\n",
    "X_test_reshaped = scaler.transform(X_test_reshaped)\n",
    "\n",
    "# Reshape